import os
import time
import requests
import csv
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

# ——— Configuration ———
QUERY      = "Software Engineering intern"
LOCATION   = "Remote"
NUM_JOBS   = 10
CSV_FILE   = "all_jobs.csv"
EXCEL_FILE = "all_jobs.xlsx"
HEADERS    = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9"
}

def scrape_indeed(query, location, num_jobs):
    """Scrape Indeed using requests + BeautifulSoup."""
    url = (
        "https://www.indeed.com/jobs"
        f"?q={query.replace(' ', '+')}"
        f"&l={location.replace(' ', '+')}"
    )
    resp = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(resp.text, "html.parser")
    cards = soup.find_all("a", class_="tapItem")[:num_jobs]

    rows = []
    for item in cards:
        title = item.find("h2").get_text(strip=True) if item.find("h2") else ""
        company = (
            item.find("span", class_="companyName").get_text(strip=True)
            if item.find("span", class_="companyName") else ""
        )
        loc = (
            item.find("div", class_="companyLocation").get_text(strip=True)
            if item.find("div", class_="companyLocation") else ""
        )
        link = "https://www.indeed.com" + item["href"]
        rows.append({
            "Source": "Indeed",
            "Title": title,
            "Company": company,
            "Location": loc,
            "Link": link
        })
    return rows

def scrape_linkedin(query, location, num_jobs):
    """Scrape LinkedIn guest API using requests + BeautifulSoup."""
    rows = []
    start = 0
    while len(rows) < num_jobs:
        url = (
            "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search"
            f"?keywords={query.replace(' ', '%20')}"
            f"&location={location.replace(' ', '%20')}"
            f"&start={start}"
        )
        resp = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(resp.text, "html.parser")
        cards = soup.find_all("li")[: num_jobs - len(rows)]
        if not cards:
            break

        for li in cards:
            title_el = li.select_one("h3.base-search-card__title")
            comp_el = li.select_one("h4.base-search-card__subtitle")
            loc_el = li.select_one("span.job-search-card__location")
            link_el = li.select_one("a.base-card__full-link")
            if title_el and comp_el and loc_el and link_el:
                rows.append({
                    "Source": "LinkedIn",
                    "Title": title_el.get_text(strip=True),
                    "Company": comp_el.get_text(strip=True),
                    "Location": loc_el.get_text(strip=True),
                    "Link": link_el["href"]
                })
        start += 25
        time.sleep(1)
    return rows

def scrape_glassdoor(query, location, num_jobs):
    """Scrape Glassdoor using Selenium + BeautifulSoup."""
    chrome_opts = Options()
    chrome_opts.add_argument("--headless=new")
    chrome_opts.add_argument(f"user-agent={HEADERS['User-Agent']}")
    driver = webdriver.Chrome(options=chrome_opts)

    url = (
        "https://www.glassdoor.com/Job/jobs.htm"
        f"?sc.keyword={query.replace(' ', '+')}"
        f"&locT=C&locId=0&locKeyword={location.replace(' ', '+')}"
    )
    driver.get(url)
    time.sleep(5)  # let JavaScript render the page

    soup = BeautifulSoup(driver.page_source, "html.parser")
    ul = soup.find("ul", class_=lambda c: c and "JobsList" in c)
    rows = []
    if ul:
        items = ul.find_all("li", recursive=False)[:num_jobs]
        for li in items:
            title_tag = li.find("a", class_=lambda c: c and "JobCard_jobTitle" in c)
            comp_tag = li.find("div", class_=lambda c: c and "EmployerProfile_profileContainer" in c)
            loc_tag = li.find("div", class_=lambda c: c and "JobCard_location" in c)
            if title_tag and comp_tag and loc_tag:
                link = title_tag["href"]
                if link.startswith("/"):
                    link = "https://www.glassdoor.com" + link
                rows.append({
                    "Source": "Glassdoor",
                    "Title": title_tag.get_text(strip=True),
                    "Company": comp_tag.get_text(strip=True),
                    "Location": loc_tag.get_text(strip=True),
                    "Link": link
                })
    driver.quit()
    return rows

def save_results(all_rows):
    """Save aggregated results to CSV and Excel."""
    df = pd.DataFrame(all_rows)
    df.to_csv(CSV_FILE, index=False)
    df.to_excel(EXCEL_FILE, index=False)
    print(f"Saved {len(df)} total jobs → {CSV_FILE}, {EXCEL_FILE}")

if __name__ == "__main__":
    indeed = scrape_indeed(QUERY, LOCATION, NUM_JOBS)
    linkedin = scrape_linkedin(QUERY, LOCATION, NUM_JOBS)
    glassdoor = scrape_glassdoor(QUERY, LOCATION, NUM_JOBS)

    combined = indeed + linkedin + glassdoor
    save_results(combined)

